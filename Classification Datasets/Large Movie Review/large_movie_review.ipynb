{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(\"imdb_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"].shuffle(buffer_size = 4000)\n",
    "dataset_test_val = dataset[\"test\"].shuffle(buffer_size = 4000)\n",
    "dataset_test = dataset_test_val.take(10000)\n",
    "dataset_val  = dataset_test_val.skip(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "dataset_texts = []\n",
    "\n",
    "for sample in dataset_train.batch(32):\n",
    "    dataset_texts.extend(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_layer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=10000, output_sequence_length=800)\n",
    "vectorization_layer.adapt(dataset_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, training = False):\n",
    "    dataset = dataset.map(lambda x : (vectorization_layer([x[\"text\"]])[0], x[\"label\"]), num_parallel_calls = 8)\n",
    "    if training:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.batch(32, drop_remainder = True)\n",
    "    if not training:\n",
    "        dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "dataset_train = process_dataset(dataset_train, True)\n",
    "dataset_val = process_dataset(dataset_val)\n",
    "dataset_test = process_dataset(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskingLayer(keras.layers.Layer):\n",
    "    \n",
    "    def call(self, inp, mask):\n",
    "        \n",
    "        N = tf.reduce_sum(tf.cast(mask, tf.float32))\n",
    "        sN = tf.math.sqrt(N)\n",
    "                \n",
    "        mean_embed = tf.reduce_sum(inp, axis=-2, keepdims=False)/sN\n",
    "        return mean_embed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=[800], dtype=tf.int64)\n",
    "embedding_layer = keras.layers.Embedding(input_dim = 10000, output_dim = 120, mask_zero=True)(input_layer)\n",
    "masking_layer = MaskingLayer()(embedding_layer)\n",
    "entry = masking_layer#keras.layers.BatchNormalization(name = \"batch\")(embedding_layer)\n",
    "wide = entry\n",
    "deep = entry\n",
    "for i in range(2):\n",
    "    wide = keras.layers.Dense(400, activation=\"relu\", kernel_regularizer = keras.regularizers.l2(0.01))(wide)\n",
    "    wide = keras.layers.BatchNormalization()(wide)\n",
    "    \n",
    "for i in range(5):\n",
    "    deep = keras.layers.Dense(50, activation=\"relu\", kernel_regularizer = keras.regularizers.l2(0.01))(deep)\n",
    "    deep = keras.layers.BatchNormalization()(deep)\n",
    "    \n",
    "concat = keras.layers.Concatenate()([wide, deep])\n",
    "out = keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer = keras.regularizers.l2(0.01))(concat)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"movie_initial_weights\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = np.logspace(-10, -1, 25)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 11s 12ms/step - loss: 9.4155 - accuracy: 0.4950\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 9.4146 - accuracy: 0.4952\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 9.4163 - accuracy: 0.4989\n",
      "781/781 [==============================] - 11s 12ms/step - loss: 9.4146 - accuracy: 0.4962\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 9.4155 - accuracy: 0.4942\n",
      "781/781 [==============================] - 11s 12ms/step - loss: 9.4120 - accuracy: 0.4972\n",
      "781/781 [==============================] - 11s 11ms/step - loss: 9.4086 - accuracy: 0.4987\n",
      "781/781 [==============================] - 11s 12ms/step - loss: 9.4009 - accuracy: 0.4969\n",
      "781/781 [==============================] - 11s 12ms/step - loss: 9.3878 - accuracy: 0.4934\n",
      "781/781 [==============================] - 11s 12ms/step - loss: 9.3603 - accuracy: 0.4949\n",
      "781/781 [==============================] - 11s 11ms/step - loss: 9.3171 - accuracy: 0.5055\n",
      "781/781 [==============================] - 11s 11ms/step - loss: 9.2602 - accuracy: 0.5168\n",
      "781/781 [==============================] - 11s 11ms/step - loss: 9.1574 - accuracy: 0.5467\n",
      "781/781 [==============================] - 11s 11ms/step - loss: 8.9365 - accuracy: 0.5944\n",
      "781/781 [==============================] - 12s 13ms/step - loss: 8.4823 - accuracy: 0.6515\n",
      "781/781 [==============================] - 12s 13ms/step - loss: 7.6078 - accuracy: 0.7202\n",
      "781/781 [==============================] - 11s 12ms/step - loss: 6.1714 - accuracy: 0.7733\n",
      "781/781 [==============================] - 12s 13ms/step - loss: 4.4733 - accuracy: 0.8098\n",
      "781/781 [==============================] - 12s 13ms/step - loss: 2.7977 - accuracy: 0.8251\n",
      "781/781 [==============================] - 14s 13ms/step - loss: 1.5831 - accuracy: 0.8252\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 0.9941 - accuracy: 0.8225\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 0.8054 - accuracy: 0.8107\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 0.7530 - accuracy: 0.8042\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 1.0383 - accuracy: 0.7907\n",
      "781/781 [==============================] - 12s 12ms/step - loss: 3.5504 - accuracy: 0.7244\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in learning_rates:\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "        optimizer = keras.optimizers.Nadam(learning_rate = learning_rate)\n",
    "    )\n",
    "    model.load_weights(\"movie_initial_weights\")\n",
    "    \n",
    "    history = model.fit(dataset_train).history\n",
    "    loss = history[\"loss\"]\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rates, losses)\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01778279410038923\n"
     ]
    }
   ],
   "source": [
    "final_lr = learning_rates[np.argmin(losses)]\n",
    "print(final_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb30c4103a0>"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate = final_lr)\n",
    ")\n",
    "model.load_weights(\"movie_initial_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "781/781 [==============================] - 14s 15ms/step - loss: 0.7421 - accuracy: 0.8113 - val_loss: 1.1745 - val_accuracy: 0.6335\n",
      "Epoch 2/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3305 - accuracy: 0.8836 - val_loss: 0.5943 - val_accuracy: 0.6965\n",
      "Epoch 3/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2968 - accuracy: 0.8994 - val_loss: 0.3795 - val_accuracy: 0.8540\n",
      "Epoch 4/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2453 - accuracy: 0.9107 - val_loss: 0.3723 - val_accuracy: 0.8498\n",
      "Epoch 5/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2548 - accuracy: 0.9145 - val_loss: 1.8190 - val_accuracy: 0.6259\n",
      "Epoch 6/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2278 - accuracy: 0.9211 - val_loss: 9.5514 - val_accuracy: 0.5123\n",
      "Epoch 7/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2340 - accuracy: 0.9224 - val_loss: 0.6178 - val_accuracy: 0.7310\n",
      "Epoch 8/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2240 - accuracy: 0.9247 - val_loss: 0.4403 - val_accuracy: 0.8301\n",
      "Epoch 9/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2002 - accuracy: 0.9293 - val_loss: 0.6053 - val_accuracy: 0.8275\n",
      "Epoch 10/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1933 - accuracy: 0.9350 - val_loss: 0.6219 - val_accuracy: 0.8017\n",
      "Epoch 11/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2377 - accuracy: 0.9311 - val_loss: 3.0185 - val_accuracy: 0.5863\n",
      "Epoch 12/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1845 - accuracy: 0.9383 - val_loss: 1.5905 - val_accuracy: 0.7190\n",
      "Epoch 13/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1746 - accuracy: 0.9388 - val_loss: 0.6004 - val_accuracy: 0.8422\n",
      "Epoch 14/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1800 - accuracy: 0.9399 - val_loss: 0.9359 - val_accuracy: 0.6520\n",
      "Epoch 15/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2278 - accuracy: 0.9387 - val_loss: 0.5962 - val_accuracy: 0.8387\n",
      "Epoch 16/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3760 - accuracy: 0.9322 - val_loss: 8.8688 - val_accuracy: 0.5148\n",
      "Epoch 17/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2251 - accuracy: 0.9423 - val_loss: 5.0357 - val_accuracy: 0.5083\n",
      "Epoch 18/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1797 - accuracy: 0.9435 - val_loss: 3.5418 - val_accuracy: 0.5334\n",
      "Epoch 19/300\n",
      "781/781 [==============================] - 11s 15ms/step - loss: 0.2219 - accuracy: 0.9402 - val_loss: 2.7870 - val_accuracy: 0.5192\n",
      "Epoch 20/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1559 - accuracy: 0.9462 - val_loss: 0.6527 - val_accuracy: 0.8117\n",
      "Epoch 21/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1668 - accuracy: 0.9439 - val_loss: 0.5858 - val_accuracy: 0.8082\n",
      "Epoch 22/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1883 - accuracy: 0.9466 - val_loss: 1.1983 - val_accuracy: 0.6646\n",
      "Epoch 23/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2187 - accuracy: 0.9467 - val_loss: 0.6069 - val_accuracy: 0.8292\n",
      "Epoch 24/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1444 - accuracy: 0.9500 - val_loss: 1.6388 - val_accuracy: 0.6823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2bd6e7ac0>"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train, validation_data=dataset_val, epochs = 300,\n",
    "         callbacks=[keras.callbacks.EarlyStopping(patience = 20, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 1s 3ms/step - loss: 0.3723 - accuracy: 0.8498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37229472398757935, 0.8497596383094788]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85% accuracy on the validation set seems convincing, but the model could be regularized even more, since the test accuracy is ~95%. I will automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(reg_str):\n",
    "    input_layer = keras.layers.Input(shape=[800], dtype=tf.int64)\n",
    "    embedding_layer = keras.layers.Embedding(input_dim = 10000, output_dim = 120, mask_zero=True)(input_layer)\n",
    "    masking_layer = MaskingLayer()(embedding_layer)\n",
    "    entry = masking_layer#keras.layers.BatchNormalization(name = \"batch\")(embedding_layer)\n",
    "    wide = entry\n",
    "    deep = entry\n",
    "    for i in range(2):\n",
    "        wide = keras.layers.Dense(400, activation=\"relu\", kernel_regularizer = keras.regularizers.l2(reg_str))(wide)\n",
    "        wide = keras.layers.BatchNormalization()(wide)\n",
    "\n",
    "    for i in range(5):\n",
    "        deep = keras.layers.Dense(50, activation=\"relu\", kernel_regularizer = keras.regularizers.l2(reg_str))(deep)\n",
    "        deep = keras.layers.BatchNormalization()(deep)\n",
    "\n",
    "    concat = keras.layers.Concatenate()([wide, deep])\n",
    "    out = keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer = keras.regularizers.l2(reg_str))(concat)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_strengths = np.logspace(-2, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization coeff: 0.01, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7930688858032227\n",
      "Regularization coeff: 0.046415888336127774, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7397168874740601\n",
      "Regularization coeff: 0.21544346900318834, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7672275900840759\n",
      "Regularization coeff: 1.0, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.8432825803756714\n",
      "Regularization coeff: 4.6415888336127775, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.4971287250518799\n",
      "Regularization coeff: 21.54434690031882, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.5052083134651184\n",
      "Regularization coeff: 100.0, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.503004789352417\n",
      "Regularization coeff: 464.1588833612773, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.48965010046958923\n",
      "Regularization coeff: 2154.4346900318824, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.4993322789669037\n",
      "Regularization coeff: 10000.0, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.5106169581413269\n"
     ]
    }
   ],
   "source": [
    "for r in reg_strengths:\n",
    "    learning_rates = np.logspace(-6, -1, 10)\n",
    "    losses = []\n",
    "    \n",
    "    model = build_model(r)\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "            optimizer = keras.optimizers.Nadam(learning_rate = learning_rate)\n",
    "        )\n",
    "        model.load_weights(\"movie_initial_weights\")\n",
    "\n",
    "        history = model.fit(dataset_train, verbose = 0).history\n",
    "        loss = history[\"loss\"]\n",
    "        losses.append(loss)\n",
    "        \n",
    "    final_lr = learning_rates[np.argmin(losses)]\n",
    "    \n",
    "    model.fit(dataset_train, validation_data=dataset_val, epochs = 300, verbose = 0,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)])\n",
    "    \n",
    "    val_acc = model.evaluate(dataset_val, verbose = 0)[1]\n",
    "    \n",
    "    print(\"Regularization coeff: {}, optimal lr: {}\".format(r, final_lr))\n",
    "    print(\"\\tResult val acc:\", val_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most optimal regularization coefficient seems to lie around 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_strengths = np.linspace(0.2, 3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization coeff: 0.2, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7506677508354187\n",
      "Regularization coeff: 0.34736842105263155, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7387820482254028\n",
      "Regularization coeff: 0.49473684210526314, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.8418135643005371\n",
      "Regularization coeff: 0.6421052631578947, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7520031929016113\n",
      "Regularization coeff: 0.7894736842105263, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7689636945724487\n",
      "Regularization coeff: 0.9368421052631579, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.503004789352417\n",
      "Regularization coeff: 1.0842105263157893, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7029246687889099\n",
      "Regularization coeff: 1.2315789473684209, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7348424196243286\n",
      "Regularization coeff: 1.3789473684210525, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.49706196784973145\n",
      "Regularization coeff: 1.526315789473684, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.6966479420661926\n",
      "Regularization coeff: 1.6736842105263157, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.49706196784973145\n",
      "Regularization coeff: 1.8210526315789473, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.5041399598121643\n",
      "Regularization coeff: 1.9684210526315786, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.5497462749481201\n",
      "Regularization coeff: 2.1157894736842104, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.49706196784973145\n",
      "Regularization coeff: 2.263157894736842, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.529113233089447\n",
      "Regularization coeff: 2.4105263157894736, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.49706196784973145\n",
      "Regularization coeff: 2.557894736842105, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.49706196784973145\n",
      "Regularization coeff: 2.705263157894737, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.7554754018783569\n",
      "Regularization coeff: 2.8526315789473684, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.5029380321502686\n",
      "Regularization coeff: 3.0, optimal lr: 0.007742636826811277\n",
      "\tResult val acc: 0.707932710647583\n"
     ]
    }
   ],
   "source": [
    "for r in reg_strengths:\n",
    "    learning_rates = np.logspace(-6, -1, 10)\n",
    "    losses = []\n",
    "    \n",
    "    model = build_model(r)\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "            optimizer = keras.optimizers.Nadam(learning_rate = learning_rate)\n",
    "        )\n",
    "        model.load_weights(\"movie_initial_weights\")\n",
    "\n",
    "        history = model.fit(dataset_train, verbose = 0).history\n",
    "        loss = history[\"loss\"]\n",
    "        losses.append(loss)\n",
    "        \n",
    "    final_lr = learning_rates[np.argmin(losses)]\n",
    "    \n",
    "    model.load_weights(\"movie_initial_weights\")\n",
    "    \n",
    "    model.fit(dataset_train, validation_data=dataset_val, epochs = 300, verbose = 0,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience = 25, restore_best_weights=True)])\n",
    "    \n",
    "    val_acc = model.evaluate(dataset_val, verbose = 0)[1]\n",
    "    \n",
    "    print(\"Regularization coeff: {}, optimal lr: {}\".format(r, final_lr))\n",
    "    print(\"\\tResult val acc:\", val_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = build_model(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate = 0.007742)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "781/781 [==============================] - 13s 14ms/step - loss: 4.5293 - accuracy: 0.5014 - val_loss: 0.8323 - val_accuracy: 0.5029\n",
      "Epoch 2/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7997 - accuracy: 0.4948 - val_loss: 0.8059 - val_accuracy: 0.5029\n",
      "Epoch 3/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.8563 - accuracy: 0.5056 - val_loss: 0.8005 - val_accuracy: 0.5029\n",
      "Epoch 4/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7367 - accuracy: 0.7510 - val_loss: 1.1745 - val_accuracy: 0.6227\n",
      "Epoch 5/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4882 - accuracy: 0.8807 - val_loss: 1.3054 - val_accuracy: 0.6838\n",
      "Epoch 6/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3982 - accuracy: 0.9023 - val_loss: 0.6814 - val_accuracy: 0.7975\n",
      "Epoch 7/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3769 - accuracy: 0.9112 - val_loss: 0.5685 - val_accuracy: 0.8030\n",
      "Epoch 8/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3485 - accuracy: 0.9162 - val_loss: 0.8533 - val_accuracy: 0.7598\n",
      "Epoch 9/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3181 - accuracy: 0.9219 - val_loss: 1.0724 - val_accuracy: 0.6781\n",
      "Epoch 10/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3082 - accuracy: 0.9225 - val_loss: 0.7145 - val_accuracy: 0.7486\n",
      "Epoch 11/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2870 - accuracy: 0.9299 - val_loss: 0.4893 - val_accuracy: 0.8220\n",
      "Epoch 12/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2934 - accuracy: 0.9285 - val_loss: 0.5494 - val_accuracy: 0.7643\n",
      "Epoch 13/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2836 - accuracy: 0.9327 - val_loss: 2.5955 - val_accuracy: 0.5202\n",
      "Epoch 14/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2475 - accuracy: 0.9352 - val_loss: 0.5038 - val_accuracy: 0.7979\n",
      "Epoch 15/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2457 - accuracy: 0.9374 - val_loss: 1.4632 - val_accuracy: 0.6010\n",
      "Epoch 16/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2366 - accuracy: 0.9396 - val_loss: 0.5427 - val_accuracy: 0.8563\n",
      "Epoch 17/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2390 - accuracy: 0.9406 - val_loss: 0.4592 - val_accuracy: 0.8527\n",
      "Epoch 18/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2293 - accuracy: 0.9426 - val_loss: 0.8651 - val_accuracy: 0.7743\n",
      "Epoch 19/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2342 - accuracy: 0.9429 - val_loss: 0.5957 - val_accuracy: 0.8414\n",
      "Epoch 20/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2419 - accuracy: 0.9433 - val_loss: 1.9876 - val_accuracy: 0.5324\n",
      "Epoch 21/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.2262 - accuracy: 0.9435 - val_loss: 0.8077 - val_accuracy: 0.7529\n",
      "Epoch 22/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2145 - accuracy: 0.9445 - val_loss: 0.7646 - val_accuracy: 0.8172\n",
      "Epoch 23/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2096 - accuracy: 0.9466 - val_loss: 1.0213 - val_accuracy: 0.7423\n",
      "Epoch 24/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2433 - accuracy: 0.9480 - val_loss: 0.5281 - val_accuracy: 0.7875\n",
      "Epoch 25/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2135 - accuracy: 0.9478 - val_loss: 0.5732 - val_accuracy: 0.7321\n",
      "Epoch 26/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2027 - accuracy: 0.9469 - val_loss: 0.7156 - val_accuracy: 0.7932\n",
      "Epoch 27/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.1952 - accuracy: 0.9516 - val_loss: 0.4997 - val_accuracy: 0.8143\n",
      "Epoch 28/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2121 - accuracy: 0.9519 - val_loss: 1.9328 - val_accuracy: 0.6347\n",
      "Epoch 29/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2305 - accuracy: 0.9467 - val_loss: 2.4415 - val_accuracy: 0.6233\n",
      "Epoch 30/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1913 - accuracy: 0.9514 - val_loss: 4.6408 - val_accuracy: 0.5508\n",
      "Epoch 31/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2135 - accuracy: 0.9523 - val_loss: 0.4346 - val_accuracy: 0.8456\n",
      "Epoch 32/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1976 - accuracy: 0.9508 - val_loss: 0.7207 - val_accuracy: 0.8500\n",
      "Epoch 33/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1760 - accuracy: 0.9571 - val_loss: 0.9134 - val_accuracy: 0.8255\n",
      "Epoch 34/300\n",
      "781/781 [==============================] - 11s 15ms/step - loss: 0.2149 - accuracy: 0.9531 - val_loss: 0.4921 - val_accuracy: 0.7874\n",
      "Epoch 35/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1979 - accuracy: 0.9539 - val_loss: 0.7734 - val_accuracy: 0.8044\n",
      "Epoch 36/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1822 - accuracy: 0.9567 - val_loss: 0.5128 - val_accuracy: 0.8293\n",
      "Epoch 37/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1861 - accuracy: 0.9568 - val_loss: 0.6772 - val_accuracy: 0.8314\n",
      "Epoch 38/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1797 - accuracy: 0.9569 - val_loss: 0.6916 - val_accuracy: 0.8428\n",
      "Epoch 39/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3260 - accuracy: 0.9497 - val_loss: 0.8598 - val_accuracy: 0.6840\n",
      "Epoch 40/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2650 - accuracy: 0.9512 - val_loss: 3.6633 - val_accuracy: 0.5763\n",
      "Epoch 41/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2696 - accuracy: 0.9489 - val_loss: 0.6236 - val_accuracy: 0.7948\n",
      "Epoch 42/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2029 - accuracy: 0.9527 - val_loss: 1.2108 - val_accuracy: 0.7356\n",
      "Epoch 43/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1887 - accuracy: 0.9533 - val_loss: 0.6302 - val_accuracy: 0.8330\n",
      "Epoch 44/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1920 - accuracy: 0.9517 - val_loss: 0.4544 - val_accuracy: 0.8467\n",
      "Epoch 45/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2011 - accuracy: 0.9550 - val_loss: 0.6550 - val_accuracy: 0.8305\n",
      "Epoch 46/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1975 - accuracy: 0.9559 - val_loss: 0.6489 - val_accuracy: 0.8389\n",
      "Epoch 47/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1811 - accuracy: 0.9547 - val_loss: 1.7045 - val_accuracy: 0.7138\n",
      "Epoch 48/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1742 - accuracy: 0.9587 - val_loss: 0.5003 - val_accuracy: 0.8385\n",
      "Epoch 49/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2705 - accuracy: 0.9504 - val_loss: 1.8181 - val_accuracy: 0.7161\n",
      "Epoch 50/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1795 - accuracy: 0.9568 - val_loss: 3.3315 - val_accuracy: 0.5455\n",
      "Epoch 51/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2298 - accuracy: 0.9503 - val_loss: 1.1702 - val_accuracy: 0.6167\n",
      "Epoch 52/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1924 - accuracy: 0.9539 - val_loss: 0.6056 - val_accuracy: 0.8360\n",
      "Epoch 53/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1904 - accuracy: 0.9529 - val_loss: 0.5073 - val_accuracy: 0.8334\n",
      "Epoch 54/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.1864 - accuracy: 0.9545 - val_loss: 0.5796 - val_accuracy: 0.8274\n",
      "Epoch 55/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1850 - accuracy: 0.9551 - val_loss: 0.5720 - val_accuracy: 0.8395\n",
      "Epoch 56/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1894 - accuracy: 0.9558 - val_loss: 1.6076 - val_accuracy: 0.6916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb27fa60250>"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(dataset_train, validation_data=dataset_val, epochs = 300, verbose = 1,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience = 25, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 1s 3ms/step - loss: 0.4346 - accuracy: 0.8456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43464744091033936, 0.8455528616905212]"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.evaluate(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still overfitting. Seems like a time for a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=[800], dtype=tf.int64)\n",
    "embedding_layer = keras.layers.Embedding(input_dim = 10000, output_dim = 120, mask_zero=True)(input_layer)\n",
    "masking_layer = MaskingLayer()(embedding_layer)\n",
    "entry = masking_layer#keras.layers.BatchNormalization(name = \"batch\")(embedding_layer)\n",
    "wide = entry\n",
    "deep = entry\n",
    "for i in range(2):\n",
    "    wide = keras.layers.Dense(400, activation=\"relu\", kernel_regularizer = keras.regularizers.l2(0.5))(wide)\n",
    "    wide = keras.layers.BatchNormalization()(wide)\n",
    "    wide = keras.layers.Dropout(0.3)(wide)\n",
    "    \n",
    "for i in range(5):\n",
    "    deep = keras.layers.Dense(50, activation=\"relu\", kernel_regularizer = keras.regularizers.l2(0.5))(deep)\n",
    "    deep = keras.layers.BatchNormalization()(deep)\n",
    "    if i > 2:\n",
    "        deep = keras.layers.Dropout(0.2)(deep)\n",
    "    \n",
    "concat = keras.layers.Concatenate()([wide, deep])\n",
    "out = keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer = keras.regularizers.l2(0.5))(concat)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate = 0.007742)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "781/781 [==============================] - 13s 14ms/step - loss: 4.5435 - accuracy: 0.5020 - val_loss: 0.7874 - val_accuracy: 0.5029\n",
      "Epoch 2/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.8056 - accuracy: 0.4994 - val_loss: 0.7882 - val_accuracy: 0.5029\n",
      "Epoch 3/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.9843 - accuracy: 0.6171 - val_loss: 0.7407 - val_accuracy: 0.7754\n",
      "Epoch 4/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7619 - accuracy: 0.8506 - val_loss: 0.6633 - val_accuracy: 0.8231\n",
      "Epoch 5/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.5994 - accuracy: 0.8860 - val_loss: 0.9661 - val_accuracy: 0.6882\n",
      "Epoch 6/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.5384 - accuracy: 0.8986 - val_loss: 0.8082 - val_accuracy: 0.7464\n",
      "Epoch 7/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.5089 - accuracy: 0.9062 - val_loss: 2.2425 - val_accuracy: 0.7563\n",
      "Epoch 8/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.4935 - accuracy: 0.9117 - val_loss: 0.5304 - val_accuracy: 0.8683\n",
      "Epoch 9/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4960 - accuracy: 0.9116 - val_loss: 0.5521 - val_accuracy: 0.8570\n",
      "Epoch 10/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4611 - accuracy: 0.9149 - val_loss: 0.5530 - val_accuracy: 0.8592\n",
      "Epoch 11/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4539 - accuracy: 0.9200 - val_loss: 0.9362 - val_accuracy: 0.7612\n",
      "Epoch 12/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4601 - accuracy: 0.9251 - val_loss: 1.0151 - val_accuracy: 0.6870\n",
      "Epoch 13/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4161 - accuracy: 0.9237 - val_loss: 0.6957 - val_accuracy: 0.8432\n",
      "Epoch 14/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4116 - accuracy: 0.9269 - val_loss: 0.6508 - val_accuracy: 0.8491\n",
      "Epoch 15/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3943 - accuracy: 0.9317 - val_loss: 0.6053 - val_accuracy: 0.8553\n",
      "Epoch 16/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3967 - accuracy: 0.9296 - val_loss: 0.5495 - val_accuracy: 0.8307\n",
      "Epoch 17/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3872 - accuracy: 0.9270 - val_loss: 0.7624 - val_accuracy: 0.7521\n",
      "Epoch 18/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3650 - accuracy: 0.9267 - val_loss: 1.9563 - val_accuracy: 0.6573\n",
      "Epoch 19/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3215 - accuracy: 0.9270 - val_loss: 0.6310 - val_accuracy: 0.8552\n",
      "Epoch 20/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3501 - accuracy: 0.9269 - val_loss: 0.4775 - val_accuracy: 0.8494\n",
      "Epoch 21/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3172 - accuracy: 0.9284 - val_loss: 2.1516 - val_accuracy: 0.5888\n",
      "Epoch 22/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3186 - accuracy: 0.9280 - val_loss: 1.2984 - val_accuracy: 0.6269\n",
      "Epoch 23/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3267 - accuracy: 0.9243 - val_loss: 0.6481 - val_accuracy: 0.7376\n",
      "Epoch 24/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3533 - accuracy: 0.9036 - val_loss: 1.8076 - val_accuracy: 0.6327\n",
      "Epoch 25/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3502 - accuracy: 0.8986 - val_loss: 0.8776 - val_accuracy: 0.8089\n",
      "Epoch 26/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3108 - accuracy: 0.9117 - val_loss: 1.0339 - val_accuracy: 0.7614\n",
      "Epoch 27/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3149 - accuracy: 0.9102 - val_loss: 1.6944 - val_accuracy: 0.6347\n",
      "Epoch 28/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3134 - accuracy: 0.9143 - val_loss: 0.6590 - val_accuracy: 0.7810\n",
      "Epoch 29/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3193 - accuracy: 0.9101 - val_loss: 0.5266 - val_accuracy: 0.8480\n",
      "Epoch 30/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3467 - accuracy: 0.9088 - val_loss: 0.6595 - val_accuracy: 0.8005\n",
      "Epoch 31/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3110 - accuracy: 0.9112 - val_loss: 0.7852 - val_accuracy: 0.8297\n",
      "Epoch 32/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3221 - accuracy: 0.9099 - val_loss: 0.5977 - val_accuracy: 0.8566\n",
      "Epoch 33/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3190 - accuracy: 0.9115 - val_loss: 0.5002 - val_accuracy: 0.8528\n",
      "Epoch 34/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3251 - accuracy: 0.9101 - val_loss: 1.7781 - val_accuracy: 0.6339\n",
      "Epoch 35/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3166 - accuracy: 0.9107 - val_loss: 0.5869 - val_accuracy: 0.8371\n",
      "Epoch 36/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3154 - accuracy: 0.9127 - val_loss: 0.5734 - val_accuracy: 0.8382\n",
      "Epoch 37/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3089 - accuracy: 0.9171 - val_loss: 0.7851 - val_accuracy: 0.7715\n",
      "Epoch 38/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.3042 - accuracy: 0.9156 - val_loss: 1.3200 - val_accuracy: 0.6927\n",
      "Epoch 39/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3063 - accuracy: 0.9174 - val_loss: 0.5855 - val_accuracy: 0.8504\n",
      "Epoch 40/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3938 - accuracy: 0.8614 - val_loss: 0.7153 - val_accuracy: 0.8361\n",
      "Epoch 41/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4070 - accuracy: 0.8380 - val_loss: 0.4572 - val_accuracy: 0.8502\n",
      "Epoch 42/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3867 - accuracy: 0.8424 - val_loss: 0.6048 - val_accuracy: 0.8073\n",
      "Epoch 43/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4969 - accuracy: 0.8231 - val_loss: 0.5143 - val_accuracy: 0.8466\n",
      "Epoch 44/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3793 - accuracy: 0.8381 - val_loss: 0.9917 - val_accuracy: 0.7380\n",
      "Epoch 45/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3791 - accuracy: 0.8371 - val_loss: 1.4497 - val_accuracy: 0.6997\n",
      "Epoch 46/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3946 - accuracy: 0.8389 - val_loss: 1.0234 - val_accuracy: 0.6735\n",
      "Epoch 47/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.3797 - accuracy: 0.8411 - val_loss: 0.4486 - val_accuracy: 0.8420\n",
      "Epoch 48/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.3909 - accuracy: 0.8370 - val_loss: 1.1180 - val_accuracy: 0.7983\n",
      "Epoch 49/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.3848 - accuracy: 0.8395 - val_loss: 0.5407 - val_accuracy: 0.8491\n",
      "Epoch 50/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3902 - accuracy: 0.8360 - val_loss: 0.5643 - val_accuracy: 0.8462\n",
      "Epoch 51/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3861 - accuracy: 0.8436 - val_loss: 0.4971 - val_accuracy: 0.8465\n",
      "Epoch 52/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3871 - accuracy: 0.8381 - val_loss: 0.7906 - val_accuracy: 0.7099\n",
      "Epoch 53/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.3928 - accuracy: 0.8400 - val_loss: 0.5817 - val_accuracy: 0.7750\n",
      "Epoch 54/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3895 - accuracy: 0.8418 - val_loss: 0.5476 - val_accuracy: 0.7879\n",
      "Epoch 55/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3910 - accuracy: 0.8390 - val_loss: 0.5983 - val_accuracy: 0.8455\n",
      "Epoch 56/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3889 - accuracy: 0.8425 - val_loss: 0.6589 - val_accuracy: 0.8355\n",
      "Epoch 57/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3898 - accuracy: 0.8386 - val_loss: 0.4607 - val_accuracy: 0.8539\n",
      "Epoch 58/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4783 - accuracy: 0.8376 - val_loss: 9.3913 - val_accuracy: 0.4987\n",
      "Epoch 59/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3584 - accuracy: 0.9125 - val_loss: 1.4817 - val_accuracy: 0.6205\n",
      "Epoch 60/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3079 - accuracy: 0.9160 - val_loss: 1.0728 - val_accuracy: 0.7679\n",
      "Epoch 61/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.4209 - accuracy: 0.8500 - val_loss: 0.5618 - val_accuracy: 0.8450\n",
      "Epoch 62/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3980 - accuracy: 0.8376 - val_loss: 0.6009 - val_accuracy: 0.7145\n",
      "Epoch 63/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3792 - accuracy: 0.8448 - val_loss: 0.6459 - val_accuracy: 0.7318\n",
      "Epoch 64/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3773 - accuracy: 0.8425 - val_loss: 0.6623 - val_accuracy: 0.8357\n",
      "Epoch 65/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3712 - accuracy: 0.8446 - val_loss: 0.6516 - val_accuracy: 0.7827\n",
      "Epoch 66/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3803 - accuracy: 0.8422 - val_loss: 0.5775 - val_accuracy: 0.8052\n",
      "Epoch 67/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.5866 - accuracy: 0.8175 - val_loss: 0.8058 - val_accuracy: 0.8084\n",
      "Epoch 68/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3974 - accuracy: 0.9426 - val_loss: 3.2200 - val_accuracy: 0.5143\n",
      "Epoch 69/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.3087 - accuracy: 0.9473 - val_loss: 0.6419 - val_accuracy: 0.7861\n",
      "Epoch 70/300\n",
      "781/781 [==============================] - 11s 15ms/step - loss: 0.2775 - accuracy: 0.9517 - val_loss: 0.8016 - val_accuracy: 0.7788\n",
      "Epoch 71/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2435 - accuracy: 0.9528 - val_loss: 0.6823 - val_accuracy: 0.7737\n",
      "Epoch 72/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2778 - accuracy: 0.9538 - val_loss: 0.5421 - val_accuracy: 0.8060\n",
      "Epoch 73/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2486 - accuracy: 0.9513 - val_loss: 8.5180 - val_accuracy: 0.8338\n",
      "Epoch 74/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.3126 - accuracy: 0.9564 - val_loss: 1.3653 - val_accuracy: 0.6313\n",
      "Epoch 75/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2267 - accuracy: 0.9619 - val_loss: 1.0263 - val_accuracy: 0.7564\n",
      "Epoch 76/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2508 - accuracy: 0.9621 - val_loss: 0.5382 - val_accuracy: 0.8438\n",
      "Epoch 77/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1987 - accuracy: 0.9633 - val_loss: 1.1242 - val_accuracy: 0.8373\n",
      "Epoch 78/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2257 - accuracy: 0.9610 - val_loss: 1.0778 - val_accuracy: 0.7903\n",
      "Epoch 79/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2123 - accuracy: 0.9609 - val_loss: 2.9187 - val_accuracy: 0.6966\n",
      "Epoch 80/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2565 - accuracy: 0.9610 - val_loss: 2.1344 - val_accuracy: 0.6337\n",
      "Epoch 81/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2966 - accuracy: 0.9637 - val_loss: 0.5692 - val_accuracy: 0.8400\n",
      "Epoch 82/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.2456 - accuracy: 0.9613 - val_loss: 1.7829 - val_accuracy: 0.7033\n",
      "Epoch 83/300\n",
      "781/781 [==============================] - 11s 13ms/step - loss: 0.2143 - accuracy: 0.9649 - val_loss: 0.5444 - val_accuracy: 0.8398\n",
      "Epoch 84/300\n",
      "781/781 [==============================] - 11s 15ms/step - loss: 0.1966 - accuracy: 0.9638 - val_loss: 0.8732 - val_accuracy: 0.7957\n",
      "Epoch 85/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.2097 - accuracy: 0.9625 - val_loss: 1.3659 - val_accuracy: 0.7524\n",
      "Epoch 86/300\n",
      "781/781 [==============================] - 11s 15ms/step - loss: 0.1979 - accuracy: 0.9655 - val_loss: 0.6451 - val_accuracy: 0.8071\n",
      "Epoch 87/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1959 - accuracy: 0.9643 - val_loss: 0.8324 - val_accuracy: 0.8202\n",
      "Epoch 88/300\n",
      "781/781 [==============================] - 12s 15ms/step - loss: 0.2197 - accuracy: 0.9634 - val_loss: 0.7167 - val_accuracy: 0.7626\n",
      "Epoch 89/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1859 - accuracy: 0.9678 - val_loss: 1.1726 - val_accuracy: 0.8104\n",
      "Epoch 90/300\n",
      "781/781 [==============================] - 11s 15ms/step - loss: 0.2763 - accuracy: 0.9598 - val_loss: 0.5302 - val_accuracy: 0.8247\n",
      "Epoch 91/300\n",
      "781/781 [==============================] - 11s 14ms/step - loss: 0.1802 - accuracy: 0.9650 - val_loss: 4.1164 - val_accuracy: 0.6287\n",
      "Epoch 92/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.1896 - accuracy: 0.9633 - val_loss: 1.4427 - val_accuracy: 0.6986\n",
      "Epoch 93/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.5174 - accuracy: 0.7773 - val_loss: 1.7152 - val_accuracy: 0.4971\n",
      "Epoch 94/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7726 - accuracy: 0.5006 - val_loss: 0.7575 - val_accuracy: 0.5029\n",
      "Epoch 95/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7530 - accuracy: 0.5013 - val_loss: 0.7437 - val_accuracy: 0.4971\n",
      "Epoch 96/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7391 - accuracy: 0.4989 - val_loss: 0.7303 - val_accuracy: 0.4971\n",
      "Epoch 97/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7298 - accuracy: 0.4990 - val_loss: 0.7287 - val_accuracy: 0.4971\n",
      "Epoch 98/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7318 - accuracy: 0.4973 - val_loss: 0.7290 - val_accuracy: 0.5029\n",
      "Epoch 99/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7293 - accuracy: 0.5035 - val_loss: 0.8053 - val_accuracy: 0.4971\n",
      "Epoch 100/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7297 - accuracy: 0.4995 - val_loss: 0.7175 - val_accuracy: 0.4971\n",
      "Epoch 101/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7223 - accuracy: 0.4934 - val_loss: 0.7202 - val_accuracy: 0.4971\n",
      "Epoch 102/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7280 - accuracy: 0.5048 - val_loss: 0.7231 - val_accuracy: 0.4971\n",
      "Epoch 103/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7251 - accuracy: 0.5036 - val_loss: 0.7289 - val_accuracy: 0.4971\n",
      "Epoch 104/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7243 - accuracy: 0.5009 - val_loss: 0.7237 - val_accuracy: 0.5029\n",
      "Epoch 105/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7303 - accuracy: 0.5022 - val_loss: 1.6881 - val_accuracy: 0.4971\n",
      "Epoch 106/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7406 - accuracy: 0.5019 - val_loss: 0.7253 - val_accuracy: 0.4971\n",
      "Epoch 107/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7243 - accuracy: 0.4982 - val_loss: 0.7224 - val_accuracy: 0.5029\n",
      "Epoch 108/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7237 - accuracy: 0.4976 - val_loss: 0.7201 - val_accuracy: 0.5029\n",
      "Epoch 109/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7267 - accuracy: 0.4970 - val_loss: 0.7224 - val_accuracy: 0.4971\n",
      "Epoch 110/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7349 - accuracy: 0.5010 - val_loss: 2.4039 - val_accuracy: 0.4971\n",
      "Epoch 111/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7356 - accuracy: 0.5013 - val_loss: 0.7289 - val_accuracy: 0.4971\n",
      "Epoch 112/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7269 - accuracy: 0.5022 - val_loss: 0.7189 - val_accuracy: 0.5029\n",
      "Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7362 - accuracy: 0.4998 - val_loss: 0.7305 - val_accuracy: 0.5029\n",
      "Epoch 114/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7253 - accuracy: 0.4985 - val_loss: 0.7270 - val_accuracy: 0.4971\n",
      "Epoch 115/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7278 - accuracy: 0.5030 - val_loss: 0.7295 - val_accuracy: 0.4971\n",
      "Epoch 116/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7239 - accuracy: 0.4950 - val_loss: 0.7178 - val_accuracy: 0.5029\n",
      "Epoch 117/300\n",
      "781/781 [==============================] - 10s 13ms/step - loss: 0.7372 - accuracy: 0.4960 - val_loss: 0.7245 - val_accuracy: 0.4971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb28d1b1280>"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train, validation_data=dataset_val, epochs = 300, verbose = 1,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience = 70, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 1s 3ms/step - loss: 0.4486 - accuracy: 0.8420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44862475991249084, 0.8420138955116272]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.8369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44628095626831055, 0.8369390964508057]"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
