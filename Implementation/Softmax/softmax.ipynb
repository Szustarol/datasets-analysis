{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the MNIST dataset from Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version = 1, return_X_y = True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLogistic(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        cost = 0\n",
    "        if self.reg == \"l1\":\n",
    "            cost += np.sum(np.abs(self.w))*self.reg_weight\n",
    "        elif self.reg == \"l2\":\n",
    "            cost += np.sum(np.square(self.w))*self.reg_weight/2\n",
    "        \n",
    "        ypadmul = self.p(X)[np.arange(0, y.shape[0]), y.flatten()]\n",
    "        cnorm = -np.sum(np.log(ypadmul))/m\n",
    "        \n",
    "        return cost + cnorm\n",
    "        \n",
    "    def p(self, X):\n",
    "        s = X @ self.w\n",
    "        saxmax = np.max(s, axis=1)\n",
    "        s -= saxmax.reshape(-1, 1)\n",
    "        sexp = np.exp(s)\n",
    "        sexpsum = np.sum(sexp, axis=1)\n",
    "        smax = sexp/sexpsum.reshape(-1,1)\n",
    "        return smax\n",
    "    \n",
    "    def dw(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        p_x = self.p(X)\n",
    "        diff = p_x \n",
    "        diff[np.arange(0, y.shape[0]), y.flatten()] -= 1\n",
    "        return X.T @ diff / m\n",
    "        \n",
    "    \n",
    "    def __init__(self, reg = None, reg_weight = 1.0, learning_rate = 1e-10):\n",
    "        self.reg = reg\n",
    "        self.reg_weight = reg_weight\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if reg is None:\n",
    "            self.d_reg = lambda : 0\n",
    "        elif reg == \"l1\":\n",
    "            self.d_reg = lambda w : reg_weight * np.sign(w[1:,:])\n",
    "        elif reg == \"l2\":\n",
    "            self.d_reg = lambda w : reg_weight * w[1:,:]\n",
    "            \n",
    "    def encode_y(self, y):\n",
    "        ny = np.zeros_like(y)\n",
    "        for index, value in enumerate(y):\n",
    "            if value in self.classes:\n",
    "                ny[index] = self.classes[value]\n",
    "        return ny.astype(int)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_biased = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        X_p = self.p(X_biased)\n",
    "        y_idx = np.argmax(X_p, axis=1)\n",
    "        y = np.zeros((X.shape[0], 1), dtype=object)\n",
    "        for index, key in enumerate(y_idx):\n",
    "            y[index] = self.keys[key]\n",
    "        return y\n",
    "    \n",
    "    def fit(self, X, y, verbose = 0, delta_cost = 1e-5):\n",
    "        X_biased = np.c_[np.ones((X.shape[0],1)), X]\n",
    "        \n",
    "        #number of classes\n",
    "        self.classes = {}\n",
    "        self.keys = {}\n",
    "        for value in y:\n",
    "            if value not in self.classes:\n",
    "                self.classes[value] = len(self.classes)\n",
    "                self.keys[self.classes[value]] = value\n",
    "        self.nclasses = len(self.classes)\n",
    "        \n",
    "        y_enc = self.encode_y(y)\n",
    "                \n",
    "        self.w = np.zeros(shape=(X_biased.shape[1], self.nclasses))\n",
    "        cost = float(\"inf\")\n",
    "        if verbose > 0:\n",
    "            print(\"Training starts\")\n",
    "        while True:\n",
    "            newcost = self.cost(X_biased, y_enc)\n",
    "            if abs(newcost - cost) < delta_cost:\n",
    "                break\n",
    "            cost = newcost\n",
    "            if verbose > 0:\n",
    "                print(\"Cost: \", self.cost(X_biased, y_enc), \" \" * 20, end=\"\\r\")\n",
    "            dw = self.dw(X_biased, y_enc)\n",
    "            self.w = self.w - self.learning_rate*dw\n",
    "        if verbose > 0:\n",
    "            print(\"Training done with delta\", delta_cost)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = SoftmaxLogistic(learning_rate=0.00001, reg=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts\n",
      "Training done with delta 0.0001                \n"
     ]
    }
   ],
   "source": [
    "smax.fit(X_train, y_train, verbose=1, delta_cost=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = smax.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9094285714285715"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite good of a result for a first try. I don't really want or need to tamper with the hyperparameters, but I will now implement the early stopping version and compare the result, but first I will compare the result with softmax from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_linreg = LogisticRegression(penalty=\"l2\", C=1.0, multi_class=\"multinomial\", max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolszustakowski/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2000, multi_class='multinomial')"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib_linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 iterations is way more than my algorithm used, so I will not train more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lib = lib_linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9115428571428571"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_lib, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, this is not much better. Let's see what I can get with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class SoftmaxEarlyStopping(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, learning_rate = 1e-4):\n",
    "        self.learning_rate = 1e-4\n",
    "    \n",
    "    def encode_y(self, y):\n",
    "        ny = np.zeros_like(y)\n",
    "        for index, value in enumerate(y):\n",
    "            if value in self.classes:\n",
    "                ny[index] = self.classes[value]\n",
    "        return ny.astype(int)\n",
    "    \n",
    "    def p(self, X):\n",
    "        s = X @ self.w\n",
    "        smax = np.amax(s, axis=1).reshape(-1, 1)\n",
    "        s = s - smax\n",
    "        sexp = np.exp(s)\n",
    "        ssum = np.sum(sexp, axis=1).reshape(-1, 1)\n",
    "        return sexp/ssum\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        p_x = self.p(X)\n",
    "        c_x = p_x[np.arange(0, y.shape[0]), y.flatten()]\n",
    "        return - np.sum(np.log(c_x)) / m\n",
    "        \n",
    "    def predict(self, X, add_bias = True, encode = True):\n",
    "        if add_bias:\n",
    "            X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        else:\n",
    "            X_b = X\n",
    "        p = self.p(X)\n",
    "        y_idx = np.argmax(p, axis = 1)\n",
    "        if encode:\n",
    "            y = np.zeros(y_idx.shape, dtype=object)\n",
    "            for i, index in enumerate(y_idx):\n",
    "                y[i] = self.keys[index]\n",
    "        else:\n",
    "            y = y_idx\n",
    "        return y\n",
    "    \n",
    "    def dw(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        p_x = self.p(X)\n",
    "        p_x[np.arange(0, y.shape[0]), y.flatten()] -= 1\n",
    "        return X.T @ p_x / m\n",
    "    \n",
    "    def learning_schedule(self, epoch, decay = 0.005):\n",
    "        return self.learning_rate/(1 + decay*epoch)\n",
    "    \n",
    "    def fit(self, X, y, verbose = 0, n_violations = 20, batch_size = 1000, delta_cost = 1e-5):\n",
    "        #split the dataset into training and validation\n",
    "        X_biased = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "                \n",
    "        self.classes = {}\n",
    "        self.keys = {}\n",
    "        \n",
    "        for c in y:\n",
    "            if c not in self.classes:\n",
    "                n = len(self.classes)\n",
    "                self.classes[c] = n\n",
    "                self.keys[n] = c\n",
    "        \n",
    "        self.nclasses = len(self.classes)\n",
    "        y_enc = self.encode_y(y)\n",
    "        X, y_enc = shuffle(X, y_enc)\n",
    "        \n",
    "        X_b_train, X_b_val, y_train, y_val = train_test_split(X, y_enc, test_size=0.2)\n",
    "        self.w = np.zeros((X_b_train.shape[1], self.nclasses))        \n",
    "        cost = float(\"inf\")\n",
    "        minCost = float(\"inf\")\n",
    "        n_voil = 0\n",
    "        best_w = self.w.copy()\n",
    "        \n",
    "        epoch = 0\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print(\"Starting training\")\n",
    "            \n",
    "        while True:\n",
    "            lr = self.learning_schedule(epoch)\n",
    "            \n",
    "            epoch += 1\n",
    "            val_cost = self.cost(X_b_val, y_val)\n",
    "                        \n",
    "            sample = random.sample(range(X_b_train.shape[0]), batch_size)\n",
    "                    \n",
    "            train_part_X = X_b_train[sample, :]\n",
    "            train_part_y = y_train[sample]\n",
    "            \n",
    "            self.w = self.w - self.dw(train_part_X, train_part_y)*lr\n",
    "            \n",
    "            val_cost = self.cost(X_b_val, y_val)\n",
    "            if abs(val_cost - cost) < delta_cost:\n",
    "                if verbose > 0:\n",
    "                    print(\"Delta cost achieved. Stopping the training\")\n",
    "                break\n",
    "            \n",
    "            cost = val_cost\n",
    "            \n",
    "            if verbose > 0:\n",
    "                val_pred = self.predict(X_b_val, add_bias=False, encode = False)\n",
    "                acc = accuracy_score(val_pred, y_val)\n",
    "                print(\"Val score: {}, best score: {}, val acc: {}\".format(val_cost, minCost, acc), \" \"*50, end=\"\\r\")\n",
    "            \n",
    "            if val_cost < minCost:\n",
    "                minCost = val_cost\n",
    "                n_voil = 0\n",
    "                best_w = self.w.copy()\n",
    "            else:\n",
    "                n_voil += 1\n",
    "                \n",
    "            if n_voil > n_violations:\n",
    "                self.w = best_w\n",
    "                if verbose > 0:\n",
    "                    print(\"\\nVal error rising, selecting best model\")\n",
    "                break\n",
    "        if verbose > 0:\n",
    "            print(\"\\nTraining done with validation error score:\", minCost)   \n",
    "            print(\"\\nFinal validation accuracy:\")\n",
    "            val_pred = self.predict(X_b_val, add_bias=False, encode = False)\n",
    "            acc = accuracy_score(val_pred, y_val)\n",
    "            print(acc)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "sme = SoftmaxEarlyStopping(learning_rate=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Val score: 0.30513130911670894, best score: 0.29816303996725174, val acc: 0.9158095238095239                                                                                                                                                                                                          \n",
      "Val error rising, selecting best model\n",
      "\n",
      "Training done with validation error score: 0.29816303996725174\n",
      "\n",
      "Final validation accuracy:\n",
      "0.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SoftmaxEarlyStopping()"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sme.fit(X_train, y_train, verbose=1, n_violations=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate this model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_estop = sme.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178285714285714"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_pred_estop, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
